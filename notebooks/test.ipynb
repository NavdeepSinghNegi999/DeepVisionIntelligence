{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "603408db-a35a-4dac-b27f-b715cbfbba56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvisual_intelligence/\\nâ”‚\\nâ”œâ”€â”€ data/\\nâ”‚   â”œâ”€â”€ raw/\\nâ”‚   â”‚   â”œâ”€â”€ Images/\\nâ”‚   â”‚   â””â”€â”€ captions.txt\\nâ”‚   â””â”€â”€ processed/\\nâ”‚       â”œâ”€â”€ df.csv\\nâ”‚       â”œâ”€â”€ train.csv\\nâ”‚       â”œâ”€â”€ val.csv\\nâ”‚       â””â”€â”€ test.csv\\nâ”‚\\nâ”œâ”€â”€ artifacts/\\nâ”‚   â”œâ”€â”€ transformer_weights.h5\\nâ”‚   â”œâ”€â”€ config_train.json\\nâ”‚   â”œâ”€â”€ history.json\\nâ”‚   â””â”€â”€ tokenizer/\\nâ”‚\\nâ”œâ”€â”€ src/\\nâ”‚   â”œâ”€â”€ data/\\nâ”‚   â”‚   â”œâ”€â”€ preprocessing.py\\nâ”‚   â”‚   â”œâ”€â”€ vectorizer.py\\nâ”‚   â”‚   â”œâ”€â”€ dataset.py\\nâ”‚   â”‚   â”œâ”€â”€ augmentation.py\\nâ”‚   â”‚   â”œâ”€â”€ save_splits.py\\nâ”‚   â”‚   â””â”€â”€ tokenizer_utils.py\\nâ”‚   â”‚\\nâ”‚   â”œâ”€â”€ models/\\nâ”‚   â”‚   â”œâ”€â”€ cnn.py\\nâ”‚   â”‚   â”œâ”€â”€ transformer.py\\nâ”‚   â”‚   â””â”€â”€ caption_model.py\\nâ”‚   â”‚\\nâ”‚   â”œâ”€â”€ training/\\nâ”‚   â”‚   â”œâ”€â”€ setup.py\\nâ”‚   â”‚   â””â”€â”€ utils.py\\nâ”‚   â”‚\\nâ”‚   â”œâ”€â”€ inference/\\nâ”‚   â”‚   â”œâ”€â”€ load_model.py\\nâ”‚   â”‚   â””â”€â”€ generate.py\\nâ”‚   â”‚\\nâ”‚   â””â”€â”€ evaluation/\\nâ”‚       â”œâ”€â”€ utils.py\\nâ”‚       â””â”€â”€ bleu.py\\nâ”‚\\nâ”œâ”€â”€ train.py\\nâ”œâ”€â”€ evaluate.py\\nâ”œâ”€â”€ infer.py\\nâ””â”€â”€ README.md\\n'"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "visual_intelligence/\n",
    "â”‚\n",
    "â”œâ”€â”€ data/\n",
    "â”‚   â”œâ”€â”€ raw/\n",
    "â”‚   â”‚   â”œâ”€â”€ Images/\n",
    "â”‚   â”‚   â””â”€â”€ captions.txt\n",
    "â”‚   â””â”€â”€ processed/\n",
    "â”‚       â”œâ”€â”€ df.csv\n",
    "â”‚       â”œâ”€â”€ train.csv\n",
    "â”‚       â”œâ”€â”€ val.csv\n",
    "â”‚       â””â”€â”€ test.csv\n",
    "â”‚\n",
    "â”œâ”€â”€ artifacts/\n",
    "â”‚   â”œâ”€â”€ transformer_weights.h5\n",
    "â”‚   â”œâ”€â”€ config_train.json\n",
    "â”‚   â”œâ”€â”€ history.json\n",
    "â”‚   â””â”€â”€ tokenizer/\n",
    "â”‚\n",
    "â”œâ”€â”€ src/\n",
    "â”‚   â”œâ”€â”€ data/\n",
    "â”‚   â”‚   â”œâ”€â”€ preprocessing.py\n",
    "â”‚   â”‚   â”œâ”€â”€ vectorizer.py\n",
    "â”‚   â”‚   â”œâ”€â”€ dataset.py\n",
    "â”‚   â”‚   â”œâ”€â”€ augmentation.py\n",
    "â”‚   â”‚   â”œâ”€â”€ save_splits.py\n",
    "â”‚   â”‚   â””â”€â”€ tokenizer_utils.py\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ models/\n",
    "â”‚   â”‚   â”œâ”€â”€ cnn.py\n",
    "â”‚   â”‚   â”œâ”€â”€ transformer.py\n",
    "â”‚   â”‚   â””â”€â”€ caption_model.py\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ training/\n",
    "â”‚   â”‚   â”œâ”€â”€ setup.py\n",
    "â”‚   â”‚   â””â”€â”€ utils.py\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ inference/\n",
    "â”‚   â”‚   â”œâ”€â”€ load_model.py\n",
    "â”‚   â”‚   â””â”€â”€ generate.py\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€â”€ evaluation/\n",
    "â”‚       â”œâ”€â”€ utils.py\n",
    "â”‚       â””â”€â”€ bleu.py\n",
    "â”‚\n",
    "â”œâ”€â”€ train.py\n",
    "â”œâ”€â”€ evaluate.py\n",
    "â”œâ”€â”€ infer.py\n",
    "â””â”€â”€ README.md\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276424a2-9ae5-4bbc-8b94-7d85ae15f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_intelligence/\n",
    "â”‚\n",
    "â”œâ”€â”€ configs/               âœ… NEW\n",
    "â”‚   â”œâ”€â”€ train_config.yaml\n",
    "â”‚   â””â”€â”€ model_config.yaml\n",
    "â”‚\n",
    "â”œâ”€â”€ data/\n",
    "â”œâ”€â”€ artifacts/\n",
    "â”œâ”€â”€ logs/                  âœ… NEW\n",
    "â”‚   â””â”€â”€ tensorboard/\n",
    "â”‚\n",
    "â”œâ”€â”€ scripts/               âœ… NEW\n",
    "â”‚   â”œâ”€â”€ run_train.sh\n",
    "â”‚   â””â”€â”€ run_inference.sh\n",
    "â”‚\n",
    "â”œâ”€â”€ src/\n",
    "â”‚   â””â”€â”€ ...\n",
    "â”‚\n",
    "â”œâ”€â”€ tests/                 âœ… NEW\n",
    "â”‚\n",
    "â”œâ”€â”€ train.py\n",
    "â”œâ”€â”€ requirements.txt       âœ… NEW\n",
    "â”œâ”€â”€ .gitignore             âœ… NEW\n",
    "â””â”€â”€ README.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "454f3961-3488-4a85-a9b1-fe255fb9ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "eb14bead-a13b-4805-bf41-ec3ab1827c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4519e5a-bcb6-4c3d-aedd-df3855f7db59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32364 4045 4046\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.data.preprocessing import preprocess_caption\n",
    "from src.training.train import split_dataset\n",
    "\n",
    "caption_path = \"../data/raw/captions.txt\" \n",
    "df = pd.read_csv(caption_path)\n",
    "\n",
    "# Preprocess captions\n",
    "df[\"caption\"] = df[\"caption\"].apply(preprocess_caption)\n",
    "\n",
    "# Split dataset (NO LEAKAGE)\n",
    "train_df, val_df, test_df = split_dataset(df)\n",
    "\n",
    "print(len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657f0b21-f53a-4247-bbad-932abd6d4283",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "12f5a6a0-49b9-4b0c-8d30-340537e6289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "seed = 999\n",
    "np.random.seed(seed)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fd3437f9-e742-4c7e-a45a-f22e4e980bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r'../'\n",
    "data_dir = os.path.join(base_dir, 'data/raw')\n",
    "img_dir = os.path.join(data_dir, 'Images')\n",
    "caption_dir = os.path.join(data_dir, 'captions.txt')\n",
    "# models_dir = os.path.join(base_dir, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf750f46-958c-4b3a-b4ff-376d762a63d7",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6b23e2f6-0ae6-4237-b304-0de44ff2a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/models/cnn.py\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import efficientnet\n",
    "\n",
    "\n",
    "def get_cnn_model(image_size=(299, 299)):\n",
    "    \"\"\"\n",
    "    Build a CNN encoder using EfficientNetB0.\n",
    "\n",
    "    The model:\n",
    "        1. Loads a pretrained EfficientNetB0 backbone.\n",
    "        2. Removes the classification head.\n",
    "        3. Freezes all CNN weights.\n",
    "        4. Reshapes spatial features into a sequence.\n",
    "\n",
    "    Args:\n",
    "        image_size (tuple): Input image size (height, width).\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: CNN encoder producing image feature sequences.\n",
    "    \"\"\"\n",
    "    base_model = efficientnet.EfficientNetB0(\n",
    "        input_shape=(*image_size, 3),\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    features = base_model.output\n",
    "    features = layers.Reshape((-1, features.shape[-1]))(features)\n",
    "\n",
    "    return keras.Model(inputs=base_model.input, outputs=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "19c639af-9060-4f90-a623-f6c5376d6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/models/caption_model.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "class ImageCaptioningModel(keras.Model):\n",
    "    \"\"\"\n",
    "    Custom Keras model for image captioning with\n",
    "    CNN encoder + Transformer encoder/decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cnn_model, encoder, decoder, image_aug=None):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.image_aug = image_aug\n",
    "\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        images, training, captions = inputs\n",
    "        features = self.cnn_model(images)\n",
    "        encoded = self.encoder(features, training=False)\n",
    "        return self.decoder(captions, encoded, training=training, mask=None)\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred, mask):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)  \n",
    "        loss *= mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
    "        acc = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
    "        acc = tf.logical_and(mask, acc)\n",
    "        acc = tf.cast(acc, tf.float32)\n",
    "        return tf.reduce_sum(acc) / tf.reduce_sum(tf.cast(mask, tf.float32))\n",
    "\n",
    "    def _compute_loss_and_acc(self, image_features, captions, training):\n",
    "        encoded = self.encoder(image_features, training=training)\n",
    "\n",
    "        seq_inp = captions[:, :-1]\n",
    "        seq_true = captions[:, 1:]\n",
    "        mask = tf.math.not_equal(seq_true, 0)\n",
    "\n",
    "        preds = self.decoder(seq_inp, encoded, training=training, mask=mask)\n",
    "\n",
    "        loss = self.calculate_loss(seq_true, preds, mask)\n",
    "        acc = self.calculate_accuracy(seq_true, preds, mask)\n",
    "\n",
    "        return loss, acc\n",
    "\n",
    "    def train_step(self, data):\n",
    "        images, captions = data\n",
    "\n",
    "        if self.image_aug:\n",
    "            images = self.image_aug(images)\n",
    "\n",
    "        image_features = self.cnn_model(images)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, acc = self._compute_loss_and_acc(image_features, captions, True)\n",
    "\n",
    "        train_vars = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        grads = tape.gradient(loss, train_vars)\n",
    "        self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state(acc)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result(), \"accuracy\": self.acc_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        images, captions = data\n",
    "        image_features = self.cnn_model(images)\n",
    "\n",
    "        loss, acc = self._compute_loss_and_acc(image_features, captions, False)\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state(acc)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result(), \"accuracy\": self.acc_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.acc_tracker]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e09ccd5b-5854-47e6-ab31-51b991c696fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/models/transformer.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer encoder block for image feature encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense = layers.Dense(embed_dim, activation=\"relu\")\n",
    "        self.norm1 = layers.LayerNormalization()\n",
    "        self.norm2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, training, mask=None):\n",
    "        x = self.norm1(inputs)\n",
    "        x = self.dense(x)\n",
    "        attn = self.attention(query=x, value=x, key=x, training=training, attention_mask=None)\n",
    "        return self.norm2(x + attn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "be6168c6-8f89-4acf-973c-282652e3d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    \"\"\"\n",
    "    Token + positional embedding layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_embed = layers.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = layers.Embedding(sequence_length, embed_dim)\n",
    "        self.scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        positions = tf.range(tf.shape(inputs)[-1])\n",
    "        return self.token_embed(inputs) * self.scale + self.pos_embed(positions)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.not_equal(inputs, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7f650a30-f679-412f-ba4d-5e7b9c898cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer decoder block for caption generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, ff_dim, num_heads, seq_len, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = PositionalEmbedding(seq_len, vocab_size, embed_dim)\n",
    "\n",
    "        self.attn1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attn2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "\n",
    "        self.ffn1 = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.ffn2 = layers.Dense(embed_dim)\n",
    "\n",
    "        self.norm1 = layers.LayerNormalization()\n",
    "        self.norm2 = layers.LayerNormalization()\n",
    "        self.norm3 = layers.LayerNormalization()\n",
    "\n",
    "        self.dropout1 = layers.Dropout(0.3)\n",
    "        self.dropout2 = layers.Dropout(0.5)\n",
    "\n",
    "        self.classifier = layers.Dense(vocab_size, activation=\"softmax\")\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
    "        x = self.embedding(inputs)\n",
    "        causal_mask = self._causal_mask(x)\n",
    "\n",
    "        attn1 = self.attn1(query=x, value=x, key=x, attention_mask=causal_mask, training=training)\n",
    "        x = self.norm1(x + attn1)\n",
    "\n",
    "        attn2 = self.attn2(query=x, value=encoder_outputs, key=encoder_outputs, training=training)\n",
    "        x = self.norm2(x + attn2)\n",
    "\n",
    "        ffn = self.ffn2(self.dropout1(self.ffn1(x), training=training))\n",
    "        x = self.norm3(x + ffn)\n",
    "\n",
    "        return self.classifier(self.dropout2(x, training=training))\n",
    "\n",
    "    def _causal_mask(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return mask[None, :, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "58060fce-b270-4b31-8cbe-0efa90892be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/inference/load_model.py\n",
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "# from src.models.cnn import get_cnn_model\n",
    "# from src.models.transformer import TransformerEncoderBlock, TransformerDecoderBlock\n",
    "# from src.models.caption_model import ImageCaptioningModel\n",
    "\n",
    "\n",
    "def get_inference_model(config_path):\n",
    "    \"\"\"\n",
    "    Load image captioning model for inference from config.\n",
    "    \"\"\"\n",
    "    with open(config_path) as f:\n",
    "        cfg = json.load(f)\n",
    "\n",
    "    cnn = get_cnn_model()\n",
    "    encoder = TransformerEncoderBlock(cfg[\"EMBED_DIM\"], cfg[\"FF_DIM\"], cfg[\"NUM_HEADS\"])\n",
    "    decoder = TransformerDecoderBlock(\n",
    "        cfg[\"EMBED_DIM\"], cfg[\"FF_DIM\"], cfg[\"NUM_HEADS\"], cfg[\"SEQ_LENGTH\"], cfg[\"VOCAB_SIZE\"]\n",
    "    )\n",
    "\n",
    "    model = ImageCaptioningModel(cnn, encoder, decoder)\n",
    "\n",
    "    img_input = tf.keras.Input(shape=(299, 299, 3))\n",
    "    cap_input = tf.keras.Input(shape=(None,))\n",
    "    model([img_input, False, cap_input])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6037e2-9a80-45b3-a0ae-201261226f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "659fbf34-8c3f-4bc1-a5fe-df17ddc963c1",
   "metadata": {},
   "source": [
    "##  utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "79061e6b-29e0-4e5c-bd57-d138b6f5fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/data/preprocessing.py\n",
    "\n",
    "import re\n",
    "import string\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_caption(caption: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess a raw image caption for training an image captioning model.\n",
    "\n",
    "    This function performs the following steps:\n",
    "        1. Converts text to lowercase.\n",
    "        2. Expands English contractions (e.g., \"don't\" -> \"do not\").\n",
    "        3. Removes punctuation characters.\n",
    "        4. Normalizes whitespace.\n",
    "        5. Removes English stopwords and single-character tokens.\n",
    "        6. Adds sequence boundary tokens (\"startseq\" and \"endseq\").\n",
    "\n",
    "    Args:\n",
    "        caption (str): Raw caption text associated with an image.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned caption formatted as\n",
    "             \"startseq <processed caption> endseq\".\n",
    "    \"\"\"\n",
    "\n",
    "    caption = caption.lower()\n",
    "    caption = contractions.fix(caption)\n",
    "\n",
    "    caption = caption.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    caption = re.sub(r\"\\s+\", \" \", caption).strip()\n",
    "    caption = \" \".join(word for word in caption.split() if word not in STOP_WORDS and len(word) > 1)\n",
    "\n",
    "    return f\"startseq {caption} endseq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3a133d14-c452-4e22-a288-1954ca1d17c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/data/image_utils.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "IMAGE_SIZE = (299, 299)\n",
    "\n",
    "def read_image_inf(image_path: str) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Load and preprocess an image for inference.\n",
    "\n",
    "    This function:\n",
    "        1. Reads the image from disk.\n",
    "        2. Decodes it as an RGB JPEG image.\n",
    "        3. Resizes it to the model's expected input size.\n",
    "        4. Normalizes pixel values to [0, 1].\n",
    "        5. Adds a batch dimension.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Preprocessed image tensor of shape\n",
    "                   (1, height, width, 3).\n",
    "    \"\"\"\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return tf.expand_dims(img, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8e0b84ad-e5c0-4116-8ed3-8f37cfa36aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/inference/generate.py\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# from src.data.image_utils import read_image_inf\n",
    "\n",
    "\n",
    "def generate_caption(image_path: str, caption_model, tokenizer, seq_length: int, show_image: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Generate a caption for a given image using a trained captioning model.\n",
    "\n",
    "    This function:\n",
    "        1. Loads and preprocesses the image.\n",
    "        2. Extracts image features using the CNN encoder.\n",
    "        3. Iteratively predicts the next word using the decoder.\n",
    "        4. Stops when the end token is generated or max length is reached.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        caption_model: Trained image captioning model.\n",
    "        tokenizer: Text vectorization layer used during training.\n",
    "        seq_length (int): Maximum caption sequence length.\n",
    "        show_image (bool): Whether to display the image during inference.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated caption without start/end tokens.\n",
    "    \"\"\"\n",
    "    vocab = tokenizer.get_vocabulary()\n",
    "    index_to_word = dict(enumerate(vocab))\n",
    "    max_len = seq_length - 1\n",
    "\n",
    "    image = read_image_inf(image_path)\n",
    "\n",
    "    if show_image:\n",
    "        img = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=3)\n",
    "        plt.imshow(img.numpy())\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    image_features = caption_model.cnn_model(image)\n",
    "    encoded_image = caption_model.encoder(image_features, training=False)\n",
    "\n",
    "    decoded_caption = \"startseq\"\n",
    "\n",
    "    for i in range(max_len):\n",
    "        tokenized = tokenizer([decoded_caption])[:, :-1]\n",
    "        mask = tf.math.not_equal(tokenized, 0)\n",
    "\n",
    "        predictions = caption_model.decoder(tokenized, encoded_image, training=False, mask=mask)\n",
    "\n",
    "        next_token_id = np.argmax(predictions[0, i])\n",
    "        next_word = index_to_word.get(next_token_id)\n",
    "\n",
    "        if next_word == \"endseq\":\n",
    "            break\n",
    "\n",
    "        decoded_caption += \" \" + next_word\n",
    "\n",
    "    return decoded_caption.replace(\"startseq\", \"\").replace(\"[UNK]\", \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b1380eb0-b2e3-4e36-abb7-4afa79775ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/data/split.py\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def dataset_split(df, dataset_len=40000, min_token=5, max_token=25, shuffle=True, seed=42,):\n",
    "    \"\"\"\n",
    "    Filter captions by length and split dataset into train/val/test.\n",
    "\n",
    "    Returns:\n",
    "        train_df, val_df, test_df\n",
    "    \"\"\"\n",
    "\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    df = df[df[\"caption\"].apply(lambda cap: min_token <= len(cap.split()) <= max_token)].iloc[:dataset_len]\n",
    "\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=seed)\n",
    "\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=seed)\n",
    "\n",
    "    return (train_df.reset_index(drop=True),val_df.reset_index(drop=True),test_df.reset_index(drop=True),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "033028c6-5b17-4550-b0b8-4e3717809540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/data/vectorizer.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "\n",
    "def build_text_vectorizer(captions, vocab_size, seq_length):\n",
    "    \"\"\"\n",
    "    Build and adapt a TextVectorization layer on captions.\n",
    "\n",
    "    Args:\n",
    "        captions (list or pd.Series): Preprocessed captions.\n",
    "        vocab_size (int): Maximum vocabulary size.\n",
    "        seq_length (int): Output sequence length.\n",
    "\n",
    "    Returns:\n",
    "        TextVectorization: Adapted vectorization layer.\n",
    "    \"\"\"\n",
    "\n",
    "    vectorizer = TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=seq_length,)\n",
    "    vectorizer.adapt(captions)\n",
    "    return vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "27bf0400-ce09-48be-b01b-334b2a15b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ðŸ“ src/data/augmentation.py\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def get_image_augmentation():\n",
    "    \"\"\"\n",
    "    Return image augmentation pipeline used during training.\n",
    "    \"\"\"\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            keras.layers.RandomFlip(\"horizontal\"),\n",
    "            keras.layers.RandomRotation(0.2),\n",
    "            keras.layers.RandomContrast(0.3),\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "47457ffb-2db4-4d7d-b09b-23925c64f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/data/dataset.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(image_path, image_size):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image from disk.\n",
    "    \"\"\"\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, image_size)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "\n",
    "def decode_image_and_vectorize(caption, image_name, img_dir, vectorizer, image_size):\n",
    "    \"\"\"\n",
    "    Load image and vectorize caption.\n",
    "    \"\"\"\n",
    "    image_path = tf.strings.join([img_dir, image_name], separator=os.path.sep)\n",
    "\n",
    "    image = load_and_preprocess_image(image_path, image_size)\n",
    "    caption = vectorizer(caption)\n",
    "\n",
    "    return image, caption\n",
    "    \n",
    "\n",
    "def make_dataset(df, img_dir, vectorizer, image_size, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create tf.data.Dataset for image captioning.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (df[\"caption\"].values, df[\"image\"].values)\n",
    "    )\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size * 8)\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda cap, img: decode_image_and_vectorize(cap, img, img_dir, vectorizer, image_size),\n",
    "        num_parallel_calls=AUTOTUNE,\n",
    "    )\n",
    "\n",
    "    return dataset.batch(batch_size).prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ab0a3-329f-4cc8-ac77-9a15d27ceaef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "624ca0d3-807f-4654-8dd7-0f936d5d810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/train/setup.py\n",
    " \n",
    "# from src.data.augmentation import get_image_augmentation\n",
    "# from src.models.cnn import get_cnn_model\n",
    "# from src.models.transformer import (TransformerEncoderBlock, TransformerDecoderBlock, ImageCaptioningModel,)\n",
    "\n",
    "\n",
    "def build_caption_model(vocab_size, seq_length, embed_dim=512, ff_dim=512, num_heads=6,):\n",
    "    \"\"\"\n",
    "    Build and return the image captioning model.\n",
    "    \"\"\"\n",
    "    cnn_model = get_cnn_model()\n",
    "\n",
    "    encoder = TransformerEncoderBlock(embed_dim=embed_dim, dense_dim=ff_dim, num_heads=num_heads)\n",
    "\n",
    "    decoder = TransformerDecoderBlock(embed_dim=embed_dim, ff_dim=ff_dim, num_heads=num_heads, seq_len=seq_length, vocab_size=vocab_size)\n",
    "\n",
    "    return ImageCaptioningModel(cnn_model=cnn_model, encoder=encoder,decoder=decoder,image_aug=get_image_augmentation())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a083422-7d2a-4a23-8d98-3ab8cb8e77b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "279d671c-11e3-4ac6-bf31-afd2728e2dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupLearningRateSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Linear warmup learning rate schedule.\n",
    "\n",
    "    The learning rate increases linearly for `warmup_steps`,\n",
    "    then stays constant at `base_lr`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr, warmup_steps):\n",
    "        super().__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "\n",
    "        warmup_lr = self.base_lr * (step / warmup_steps)\n",
    "        return tf.cond(step < warmup_steps, lambda: warmup_lr, lambda: self.base_lr)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"base_lr\": self.base_lr,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def build_optimizer(train_dataset, epochs: int, base_lr: float = 1e-4, warmup_ratio: int = 15):\n",
    "    \"\"\"\n",
    "    Build Adam optimizer with warmup learning rate schedule.\n",
    "\n",
    "    Args:\n",
    "        train_dataset (tf.data.Dataset): Training dataset.\n",
    "        epochs (int): Total number of epochs.\n",
    "        base_lr (float): Target learning rate after warmup.\n",
    "        warmup_ratio (int): Warmup steps = total_steps / warmup_ratio.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.optimizers.Optimizer\n",
    "    \"\"\"\n",
    "    total_steps = len(train_dataset) * epochs\n",
    "    warmup_steps = total_steps // warmup_ratio\n",
    "\n",
    "    lr_schedule = WarmupLearningRateSchedule(base_lr=base_lr,warmup_steps=warmup_steps)\n",
    "\n",
    "    return keras.optimizers.Adam(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f95e64-8771-494a-87df-6b975320b6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0e1de2be-f06a-4c02-9b9c-1da5d3965801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history():\n",
    "    \"\"\"\n",
    "    Plot training loss over epochs.\n",
    "    \"\"\"\n",
    "    loss = history.history['loss']\n",
    "    epochs_range = range(1, len(loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_range, loss, label='Training Loss', marker='o', linestyle='-', color='orange')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d86e199-8e7a-4ca1-8bbf-a762af7e296c",
   "metadata": {},
   "source": [
    "##  Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "337433f3-1fc7-467f-98ab-6f1ce9515e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/training/utils.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "def save_training_artifacts(model, history, config, save_dir, weights_path):\n",
    "    \"\"\"\n",
    "    Save model weights, training history, and config.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save history\n",
    "    with open(os.path.join(save_dir, \"history.json\"), \"w\") as f:\n",
    "        json.dump(history.history, f)\n",
    "\n",
    "    # Save model weights\n",
    "    model.save_weights(weights_path)\n",
    "\n",
    "    # Save training config\n",
    "    with open(os.path.join(save_dir, \"config_train.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ff80ba9f-9d63-462f-adef-1e160b4d582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/data/tokenizer_utils.py\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def save_tokenizer(vectorizer, save_dir):\n",
    "    \"\"\"\n",
    "    Save TextVectorization layer as a model.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "    outputs = vectorizer(inputs)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    model.save(os.path.join(save_dir, \"tokenizer\"), save_format=\"tf\")\n",
    "\n",
    "\n",
    "def load_tokenizer(save_dir):\n",
    "    \"\"\"\n",
    "    Load saved TextVectorization layer.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.load_model(os.path.join(save_dir, \"tokenizer\"), compile=False)\n",
    "    return model.layers[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "14053e3a-fced-49b4-a864-5789803d3d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/data/save_splits.py\n",
    "import os\n",
    "\n",
    "\n",
    "def save_splits(df, train_df, val_df, test_df, data_dir):\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    df.to_csv(os.path.join(data_dir, \"df.csv\"), index=False)\n",
    "    train_df.to_csv(os.path.join(data_dir, \"train.csv\"), index=False)\n",
    "    val_df.to_csv(os.path.join(data_dir, \"val.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(data_dir, \"test.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "902a71c2-1168-4f9b-a015-0e8491d6e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/models/inference.py\n",
    "import json\n",
    "# from src.models.model import get_inference_model\n",
    "\n",
    "\n",
    "def load_inference_model(config_path, weights_path):\n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    model = get_inference_model(config_path)\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    return model, config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d98dc80b-7bed-41a9-ad07-8370dd94b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/evaluation/utils.py\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def build_image_caption_map(all_df, subset_df):\n",
    "    \"\"\"\n",
    "    Map image_id -> list of ground truth captions.\n",
    "    \"\"\"\n",
    "    image_caption_map = defaultdict(list)\n",
    "    image_ids = set(subset_df[\"image\"].unique())\n",
    "\n",
    "    for _, row in all_df.iterrows():\n",
    "        if row[\"image\"] in image_ids:\n",
    "            caption = (row[\"caption\"].replace(\"startseq \", \"\").replace(\" endseq\", \"\").strip())\n",
    "            image_caption_map[row[\"image\"]].append(caption)\n",
    "\n",
    "    return dict(image_caption_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "b0df6da9-8283-4dd6-ad93-e7c142a27c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/evaluation/bleu.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# from src.inference.generate import generate_caption\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu(model, tokenizer, image_caption_map):\n",
    "    \"\"\"\n",
    "    Compute BLEU-1 to BLEU-4 scores.\n",
    "    \"\"\"\n",
    "    bleu_1_scores, bleu_2_scores, bleu_3_scores, bleu_4_scores = [], [], [], []\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "    for image_id in tqdm(image_caption_map.keys(), desc=\"Calculating BLEU Scores\", unit=\"image\"):\n",
    "        image_path = os.path.join(img_dir, image_id)\n",
    "\n",
    "        predicted_caption = generate_caption(image_path, model, tokenizer, model_config[\"SEQ_LENGTH\"], False).strip()\n",
    "        predicted_caption = predicted_caption.split()\n",
    "\n",
    "        reference_captions = [caption.split() for caption in image_caption_map[image_id]]\n",
    "\n",
    "        bleu_1 = sentence_bleu(reference_captions, predicted_caption, weights=(1, 0, 0, 0), smoothing_function=smooth_fn)\n",
    "        bleu_2 = sentence_bleu(reference_captions, predicted_caption, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth_fn)\n",
    "        bleu_3 = sentence_bleu(reference_captions, predicted_caption, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth_fn)\n",
    "        bleu_4 = sentence_bleu(reference_captions, predicted_caption, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth_fn)\n",
    "\n",
    "        bleu_1_scores.append(bleu_1)\n",
    "        bleu_2_scores.append(bleu_2)\n",
    "        bleu_3_scores.append(bleu_3)\n",
    "        bleu_4_scores.append(bleu_4)\n",
    "\n",
    "    return [np.mean(bleu_1_scores), np.mean(bleu_2_scores), np.mean(bleu_3_scores), np.mean(bleu_4_scores)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "150eb051-05e1-43d3-8db8-a7635a4f2dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_train = {\n",
    "    \"IMAGE_SIZE\": IMAGE_SIZE,\n",
    "    \"SEQ_LENGTH\": SEQ_LENGTH,\n",
    "    \"EMBED_DIM\": EMBED_DIM,\n",
    "    \"NUM_HEADS\": NUM_HEADS,\n",
    "    \"FF_DIM\": FF_DIM,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "    \"VOCAB_SIZE\": VOCAB_SIZE,\n",
    "}\n",
    "\n",
    "save_training_artifacts(caption_model, history, config_train, save_dir=save_path, weights_path=weights_path)\n",
    "\n",
    "save_tokenizer(vectorization, save_path)\n",
    "save_splits(train_df, valid_df, test_df, data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f99d7-7d2a-4665-8fa8-8f27077e9424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bff60cd5-6794-42bd-856c-3c50bc5c38ea",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "df7dc06f-53bd-4cd8-ad3e-ab9618d83455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of images 8091 and total number of captions 40455\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set of stairs in an entry way .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a wooden cabin .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                                                    caption  \n",
       "0  A child in a pink dress is climbing up a set of stairs in an entry way .  \n",
       "1                                     A girl going into a wooden building .  \n",
       "2                          A little girl climbing into a wooden playhouse .  \n",
       "3                      A little girl climbing the stairs to her playhouse .  \n",
       "4                 A little girl in a pink dress going into a wooden cabin .  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(caption_dir)\n",
    "\n",
    "print(f\"Total Number of images {len(df)//5} and total number of captions {len(df)}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b49b7b75-a8cf-475a-a32c-476855edd445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq child pink dress climbing set stairs entry way endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq girl going wooden building endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq little girl climbing wooden playhouse endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq little girl climbing stairs playhouse endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq little girl pink dress going wooden cabin endseq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                                          caption  \n",
       "0  startseq child pink dress climbing set stairs entry way endseq  \n",
       "1                      startseq girl going wooden building endseq  \n",
       "2           startseq little girl climbing wooden playhouse endseq  \n",
       "3           startseq little girl climbing stairs playhouse endseq  \n",
       "4       startseq little girl pink dress going wooden cabin endseq  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['caption'] = df['caption'].apply(preprocess_caption)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c5910fd0-c327-4f54-bc45-4679ed13e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5cee8ad5-fd12-4c60-a109-9fae158141e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum caption length: 20\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = max(df['caption'].apply(lambda cap: len(cap.split())))\n",
    "print(\"Maximum caption length:\", MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8d6ebd8a-8c3a-4e79-acc2-76992e3fa253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  794\n",
      "Number of validation samples:  99\n",
      "Number of test samples:  100\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df, test_df = dataset_split(df, dataset_len=len(df))\n",
    "print(\"Number of training samples: \", len(train_df))\n",
    "print(\"Number of validation samples: \", len(valid_df))\n",
    "print(\"Number of test samples: \", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c3d180bc-80f4-4ccd-a18f-3b5609031a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1033 20\n"
     ]
    }
   ],
   "source": [
    "# caption_lengths = train_df[\"caption\"].apply(lambda x: len(x.split()))\n",
    "# SEQ_LENGTH = int(caption_lengths.quantile(0.95))\n",
    "# print(caption_lengths.describe())\n",
    "\n",
    "\n",
    "\n",
    "unique_words = set(\" \".join(train_df[\"caption\"]).split())\n",
    "VOCAB_SIZE = len(unique_words)\n",
    "\n",
    "SEQ_LENGTH = MAX_LEN\n",
    "print(VOCAB_SIZE, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1f32ac5b-61d5-4ec4-9ebf-327c12c40103",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = build_text_vectorizer(train_df[\"caption\"], VOCAB_SIZE, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2a54aff7-8591-446f-9a72-70b87ab9b645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word to Index Mapping (First 5 words): [('', 0), ('[UNK]', 1), ('startseq', 2), ('endseq', 3), ('dog', 4)]\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_vocabulary()\n",
    "\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "print(\"Word to Index Mapping (First 5 words):\", list(word_to_index.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6cc2d724-b806-4bb9-8682-619bf6550d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 299, 299, 3) (32, 20)\n"
     ]
    }
   ],
   "source": [
    "# src/train.py\n",
    "\n",
    "# from src.data.dataset import make_dataset\n",
    "# from src.train.setup import build_caption_model\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (299, 299)\n",
    "EPOCHS = 5\n",
    "\n",
    "train_dataset = make_dataset(train_df, img_dir, vectorizer, IMAGE_SIZE, BATCH_SIZE,shuffle=True)\n",
    "val_dataset = make_dataset(valid_df, img_dir, vectorizer, IMAGE_SIZE, BATCH_SIZE, shuffle=False)\n",
    "test_dataset = make_dataset(test_df, img_dir, vectorizer, IMAGE_SIZE, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = build_caption_model(vocab_size=VOCAB_SIZE, seq_length=SEQ_LENGTH)\n",
    "\n",
    "batch = next(iter(train_dataset))\n",
    "print(batch[0].shape, batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "78049431-ef0a-4272-a71e-b24aaa45d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction=\"none\")\n",
    "optimizer = build_optimizer(train_dataset, epochs=EPOCHS)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "594e04d6-ea76-4f8d-a4c9-d1e068c7866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "25/25 [==============================] - 9s 221ms/step - loss: 6.7740 - accuracy: 0.0804 - val_loss: 5.5513 - val_accuracy: 0.1718\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 5s 185ms/step - loss: 5.7387 - accuracy: 0.1698 - val_loss: 5.1883 - val_accuracy: 0.1945\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 5s 187ms/step - loss: 5.3045 - accuracy: 0.1982 - val_loss: 5.0124 - val_accuracy: 0.2113\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 5s 187ms/step - loss: 4.9787 - accuracy: 0.2167 - val_loss: 4.8796 - val_accuracy: 0.2186\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 5s 186ms/step - loss: 4.6473 - accuracy: 0.2434 - val_loss: 4.7295 - val_accuracy: 0.2317\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=EPOCHS, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4b7ea5c2-ef26-42b6-af22-403d955d32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 512\n",
    "FF_DIM = 512\n",
    "NUM_HEADS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9356d5aa-5134-460d-84c7-c2e495583a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.training.utils import save_training_artifacts\n",
    "\n",
    "save_dir = os.path.join(base_dir, \"artifacts\")\n",
    "weights_path = os.path.join(save_dir, \"transformer_weights.h5\")\n",
    "\n",
    "config = {\n",
    "    \"IMAGE_SIZE\": IMAGE_SIZE,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "    \"SEQ_LENGTH\": SEQ_LENGTH,\n",
    "    \"VOCAB_SIZE\": VOCAB_SIZE,\n",
    "    \"EMBED_DIM\": EMBED_DIM,\n",
    "    \"FF_DIM\": FF_DIM,\n",
    "    \"NUM_HEADS\": NUM_HEADS,\n",
    "}\n",
    "\n",
    "save_training_artifacts(model=model, history=history, config=config, save_dir=save_dir, weights_path=weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b69b7cf1-beaf-47ab-83f7-90afe1b4b94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../artifacts\\tokenizer\\assets\n"
     ]
    }
   ],
   "source": [
    "save_tokenizer(vectorizer, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "310e49b3-ac51-4251-94c2-2aeccd3ee287",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(save_dir, \"data\")\n",
    "save_splits(df, train_df, valid_df, test_df, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd1da5-1bd6-462c-adba-dc4d2288aba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e60a09ce-763c-4550-b094-0edd1fc56f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tokenizer = load_tokenizer(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "1fa51e28-c4b8-45bc-891c-f7dc8381bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_config_path = os.path.join(save_dir, 'config_train.json')\n",
    "get_model_weights_path = os.path.join(save_dir, 'transformer_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c4ea856f-368c-468a-be5d-e162342642e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model, loaded_config = load_inference_model(get_model_config_path, get_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a110b11-83ed-4c1a-9217-35c97d2edab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f2462-45ad-460c-b9fa-3d7dd4a0b449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12499b7b-d93e-479c-8553-28f46dcfcb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902b6cd-a0b2-436c-8aa4-cad2965d4c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1f5247e8-dbcc-4459-8f79-b7342f705b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_dir, 'df.csv'))\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "\n",
    "\n",
    "train_image_caption_map = build_image_caption_map(df, train_df)\n",
    "test_image_caption_map = build_image_caption_map(df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "110e613b-ea17-45a9-8498-2a1637721882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BLEU Scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:40<00:00,  4.90image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU-1 Score on Train Dataset: 0.3878\n",
      "Average BLEU-2 Score on Train Dataset: 0.2225\n",
      "Average BLEU-3 Score on Train Dataset: 0.1496\n",
      "Average BLEU-4 Score on Train Dataset: 0.1119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu(model, tokenizer, train_image_caption_map)\n",
    "print(f\"Average BLEU-1 Score on Train Dataset: {bleu_score[0]:.4f}\")\n",
    "print(f\"Average BLEU-2 Score on Train Dataset: {bleu_score[1]:.4f}\")\n",
    "print(f\"Average BLEU-3 Score on Train Dataset: {bleu_score[2]:.4f}\")\n",
    "print(f\"Average BLEU-4 Score on Train Dataset: {bleu_score[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "7a80372e-4930-4d84-8b5b-82f360c01e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BLEU Scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [00:16<00:00,  4.91image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU-1 Score on Test Dataset: 0.3981\n",
      "Average BLEU-2 Score on Test Dataset: 0.2256\n",
      "Average BLEU-3 Score on Test Dataset: 0.1517\n",
      "Average BLEU-4 Score on Test Dataset: 0.1148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu(model, tokenizer, test_image_caption_map)\n",
    "print(f\"Average BLEU-1 Score on Test Dataset: {bleu_score[0]:.4f}\")\n",
    "print(f\"Average BLEU-2 Score on Test Dataset: {bleu_score[1]:.4f}\")  \n",
    "print(f\"Average BLEU-3 Score on Test Dataset: {bleu_score[2]:.4f}\")\n",
    "print(f\"Average BLEU-4 Score on Test Dataset: {bleu_score[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87041cf-cd6c-46a5-b143-eab37832d8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7be110-7f23-4006-a063-6e5dbacc15a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
